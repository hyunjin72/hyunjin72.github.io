---
layout: restricted
title: AI810 Blog Post (20238069)
date: 2025-05-31
password: ai810-20238069
bibliography: 2025-05-31-ai810.bib
toc:
    - name: 1 Introduction
    - name: 2 How Does Molecular Diffusion Models Work?
        subsections:
        - name: Problem definition
        - name: Forward and reverse diffusion processes
        - name: Training objective
    - name: 3 The Challenge: Different Level of Difficulties in Molecules
    - name: 4 Solution
        subsections:
        - name: Molecule-wise adaptive forward diffusion process
        - name: Maintaining gradient flow in molecular diffusion models
    - name: 5 Experimental Results
    - name: 6 Summary
---

## 1 Introduction

Diffusion models have emerged as powerful tools for generating molecules, offering a principled framework for constructing molecular graphs from scratch <d-cite key="digress, gdss, mood, grum, graphdit, grapharm, meld"></d-cite>. Given the inherently discrete nature of molecular structures, most approaches rely on discrete or masked diffusion processes that operate directly in graph space. These methods gradually corrupt molecules—typically by masking nodes and edges or altering atom and bond types—and then train a model to reverse this corruption and recover the original structure.

However, a critical assumption underlies all these methodologies: *all molecules are equally amenable to this generative process*. molecular graphs differ substantially in structural complexity, which directly affects how difficult they are to reconstruct during training. Recognizing and adapting to these differences is essential for advancing the training efficacy.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-05-31-ai810/1.gif" class="img-fluid rounded z-depth-2" %}
    </div>
</div>

## 2 How Does Molecular Diffusion Models Work?

### Problem definition

We consider the task of generating molecules represented as graphs, denoted by $\mathcal G=(\mathcal V,\mathcal E)$, where $\mathcal V$ is a set of $N$ atoms (nodes) and  $\mathcal E$ a set of bonds (edges). Here, we focus on the masked diffusion models (MDMs) as a base architecture. In MDMs, we encode $\mathcal V$ with a categorical feature $\bm{x} = (x^{i})_{i=1}^N$, where each $x^i \in \{1,\dots,A\}$ indicates the atom type of node $i$ among the $A$ possible types with one category reserved for $[\texttt{mask}]$ token. Similarly, we encode $\mathcal E$  as a categorical feature $\bm{e} = (e^{ij})_{i,j=1}^N$ where each $e^{ij} \in \{1,\dots,B\}$ is the bond type between the atoms $i$ and $j$, with two categories reserved for the "no bond" and $[\texttt{mask}]$ tokens. Since the graph is an abstract object, our implementations rely on explicit representation $\bm{g} = (\bm{x},\bm{e})$ of graph $\mathcal G$. Finally, our goal is to train a graph generator from the data distribution $q(\bm{g})$. To this end, we introduce a forward process $q(\bm{g}_{t} | \bm{g}_{t-1})$ and a reverse process $p_{\theta}(\bm{g}_{t-1}|\bm{g}_{t})$ which is parameterized by $\theta$.

### Forward and reverse diffusion processes

At each timestep $t$, the forward diffusion process $q$ progressively corrupts atoms and bonds by stochastically transitioning them into a masked state, denoted by $\texttt{[mask]}$. This transition is governed by Markov kernels defined with a monotonically increasing sequence $\beta_t\in[0,1]$. Specifically, the Markov kernels are defined as

$$
q(g_{t}^{i}\,|\,g^i_{t-1}) =
\begin{cases}
\beta_{t, \phi}^{i} &\text{ if } g_{t}^{i} = [\texttt{mask}]\\
1-\beta_{t, \phi}^{i} &\text{ if }g_{t}^{i}=g_{t-1}^{i}
\end{cases}.
$$

Next, the reverse process recovers the clean graph from a noisy state $\bm{g}_t$, and we learn this process through a model $p_\theta(\bm{g}_{t-1}|\bm{g}_t)$. In particular, each denoising step is formalized as the product over nodes and edges:

$$
p_\theta(\bm{g}_{t-1}|\bm{g}_t)=\prod_{v\in\mathcal V}p_\theta(x_{t-1}|\bm{g}_t)\prod_{e \in \mathcal E}p_\theta(e_{t-1}|\bm{g}_t).
$$

### Training objective

In practice, the denoising network is trained to directly predict the original graph $\bm{g}_0$ from the noisy intermediate state $\bm g_t$, eliminating the need for recursive sampling. This is achieved by training the model based on a cross-entropy loss between the predicted logits and the ground-truth node and edge categories:

$$
\mathcal{L} (\theta) :=-\mathbb{E}_{t, \bm{g}, \bm{g}_{t}}\bigg[ \sum_{1 \leq i \leq N} \frac{\dot{\alpha}_{t}^{i}}{1-\alpha_{t}^{i}}\log p_\theta(x^i \mid \bm{g}_t) + \lambda \sum_{1 \leq i,j \leq N} \frac{\dot{\alpha}_{t}^{ij}}{1-\alpha_{t}^{ij}}\log p_\theta(e^{ij} \mid \bm{g}_t)\bigg],
$$

where the expectation is over $\bm{g}\sim q(\cdot), \bm{g}_{t} \sim q_{\phi}(\cdot|\bm{g})$, $t\sim \operatorname{Unif}[0,1]$. Furthermore, we let $\alpha_{t}^{i}=\prod_{t'=t}^{T}\beta_{t'}^{i}$ and ${\dot{\alpha}_{t}^{i}}$ is the derivative $\alpha_{t}^{i}$ with respect to $t$, and $\lambda>0$ is a hyperparameter that balances the contributions of node- and edge-level reconstruction introduced in prior works <d-cite key="digress, graphdit, meld"></d-cite>.

## 3 The Challenge: Different Level of Difficulties in Molecules

The diverging level of training difficulty across samples has been raised in image domains <d-cite key="cl"></d-cite>. However, such an intuition has not been explored in molecular data. In fact, the similar phenomenon is also natural for molecules since their structures highly varies from simple linear chains to complex cyclic structures. And this turns out correct — when diffusion models attempt to reconstruct the complex motifs structures, we find that they often struggle to yield chemically valid molecules.

Specifically, we compare the chemical validity of cyclic and linear/branched substructures in 200 generated molecules. According to the table, cyclic structures exhibit significantly lower validity (67.3%) compared to branched structures (89.7%), indicating that the model struggles more with generating structures with complex constraints.

| **Validity** | **Cycles** | **Branches (Non-cycles)** |
| --- | --- | --- |
| MDMs | 0.6732 | 0.8970 |

In addition, we visualize the generated molecules from the model. Then we highlight chemically valid substructures as green, and invalid substructures as red. As shown, cyclic regions (left) tend to exhibit more invalid atoms and bonds (red) than simple linear regions (right), where all atoms and connections remain valid (green).

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-05-31-ai810/2.png" class="img-fluid rounded z-depth-2" %}
    </div>
</div>

Hence, this discrepancy highlights the need for a more fine-grained approach that considers the inherent difficulty associated with different molecular structures, especially to improve the generative performance on structurally complex motifs like rings.

## 4 Solution

To address this challenge, we propose integrating a molecule-wise learnable noise schedule into the diffusion process. Instead of applying a molecule-agnostic noise level across all molecular structures, we encourage the model to dynamically adjust the noise based on the complexity of the molecule.

### Molecule-wise adaptive forward diffusion process

We consider learnable molecule-wise noise scheduling network that assigns distinct noise schedules according to graph structures. Specifically, we employ a parameterized noise embedding network $f_\phi$ to map input nodes and edges to the graph-level representation as follows:

$$
f_\phi(\bm g):=\text{GNN}_\phi(\bm x,\bm e)\in\R^K
$$

Optionally, the latent representation $f_\phi(\cdot)$ can be enriched by concatenating with additional supervision $\bm c$, where $\bm c$ can be given as molecular properties, for use in property-conditioned molecular generation task.

We then parameterize the noise schedule using a weighted combination of polynomial basis function. Concretely, we use the network output as coefficients $w$ for the basis, and define the schedule as $\alpha_{t,\phi}=\sigma(-\gamma_t)$, where $\sigma(\cdot)$ is a sigmoid function and $\gamma_t$ is a noise scheduling network computed via:

$$
\gamma_t = \hat{\gamma}_t\cdot (\gamma_{\max} - \gamma_{\min}) + \gamma_{\min}, \quad \hat{\gamma}_t = \frac{\sum_{k=1}^K w_k t^k}{\sum_{k=1}^K w_k},
$$

where the coefficient $w_k$ is the $k$-th element in the output of the noise embedding network $f_\phi(\bm g)$. We fix $\gamma_{\max}=5$ and $\gamma_{\min}=-13$ for all experiments. Throughout this process, our method naturally introduces molecule-specific masking rates, adaptively controls the corruption difficulty based on graph structure, and allows the model to better accommodate structural heterogeneity during training.

### Maintaining gradient flow in molecular diffusion models

In discrete-space molecular diffusion frameworks, the noisy graph at each timestep is obtained by sampling a single graph from a categorical distribution over nodes and edges, as computing the full expectation over $\bm g_t\sim q(\cdot | \bm g)$ is intractable. However, such discretization introduces a discontinuity in the computational graph when parameterizing the forward process, impeding a gradient flow towards $q_\phi$ in our new loss objective:

$$
\mathcal{L} (\theta, \phi) :=-\mathbb{E}_{t, \bm{g}, \bm{g}_{t}}\bigg[ \sum_{1 \leq i \leq N} \frac{\dot{\alpha}_{t,\phi}^{i}}{1-\alpha_{t, \phi}^{i}}\log p_\theta(x^i \mid \bm{g}_t) \\+ \lambda \sum_{1 \leq i,j \leq N} \frac{\dot{\alpha}_{t, \phi}^{ij}}{1-\alpha_{t, \phi}^{ij}}\log p_\theta(e^{ij} \mid \bm{g}_t)\bigg],
$$

where $\dot{\alpha}_{t,\phi}$ is the derivative of ${\alpha}_{t,\phi}$ with respect to $t$.

To circumvent this issue, we adopt the Straight-Through Gumbel-Softmax (STGS) trick, which enables end-to-end training by providing a differentiable surrogate for discrete sampling. Let $\bm{z} \in \mathbb{R}^A$ denote the logits over $A$ possible node types, and $\eta > 0$ be the temperature parameter. We first compute a soft approximation of the categorical distribution $\bm{p}_{\text{soft}} \in [0, 1]^{A}$ via the Gumbel-Softmax:

$$
\bm{p}_{\text{soft}, k} = \frac{\exp((z_k + g_k)/\eta)}{\sum_{l=1}^{A} \exp((z_l + g_l)/\eta)},
$$

where $g_k = -\log(-\log(u_k))$ is a gumbel noise with $u_k \sim \text{Uniform}(0, 1)$ and $z_k$ is the $k$-th element of the logits $\bm{z}$. A discrete one-hot vector $\bm{p}_{\text{hard}} \in \{0, 1\}^{A}$ is then obtained by taking the index with the highest probability:

$$
k^* = \arg\max_{k} \bm{p}_{\text{soft}, k}, \quad \bm{p}_{\text{hard}, k} = \begin{cases}
1 & \text{if } k = k^* \\
0 & \text{otherwise}
\end{cases}
$$

To retain gradient flow, we use the straight-through estimator to combine the discrete and continuous components, *i.e.*, set $\bm{p} = \bm{p}_{\text{hard}} - \text{sg}(\bm{p}_{\text{soft}}) + \bm{p}_{\text{soft}}$, where $\text{sg}(\cdot)$ denotes the stop-gradient operator. This ensures that the forward pass uses the discretized one-hot vector $\bm{p} = \bm{p}_{\text{hard}}$, while the backward pass treats $\bm p$ as the continuous $\bm{p}_{\text{soft}}$, allowing gradients to propagate through $z$, *i.e.*, $\frac{\partial \bm{p}}{\partial z} = \frac{\partial \bm{p}_{\text{soft}}}{\partial z}$.

## 5 Experimental Results

As demonstrated in the following table, applying this adaptive approach with standard MDM architecture yields significant improvements in the chemical validity of generated molecules, particularly those with complex structures:

- **Cyclic Structures**: Marked increase in validity scores, indicating more precise reconstruction of complex molecular structures.
- **Linear/Branched Structures**: Maintained high validity, demonstrating that the adaptive approach doesn't compromise simpler structures.

| **Validity** | **Cycles** | **Branches (Non-cycles)** |
| --- | --- | --- |
| MDMs | 0.6732 | 0.8970 |
| **Ours** | **0.9878** | **0.9890** |

When evaluated 10K generated polymers upon diverse molecular metrics, our method achieves state-of-the-art performance across most metrics, particularly in distributional similarity (FCD, Frag. Sim.) and property alignment (O2, N2, CO2 permeability), highlighting its effectiveness.

| **Methods** | Validity | Diversity | Frag./Sim. | FCD | O2/MAE ↓ | N2/MAE ↓ | CO2/MAE ↓ |
| --- | --- | --- | --- | --- | --- | --- | --- |
| GraphGA | 1.0000 | 0.8828 | 0.9269 | 9.1882 | 1.9840 | 2.2900 | 1.9489 |
| MARS | 1.0000 | 0.8375 | 0.9283 | 7.5620 | 1.5761 | 1.8327 | 1.6074 |
| LSTM-HC | 0.9910 | 0.8918 | 0.7937 | 18.1562 | 1.1003 | 1.2365 | 1.0772 |
| JTVAE-BO | 1.0000 | 0.7366 | 0.7294 | 23.5990 | 1.0781 | 1.2352 | 1.0978 |
| DiGress | 0.9812 | **0.9105** | 0.2771 | 21.7311 | 1.7130 | 2.0632 | 1.6648 |
| GDSS | 0.9205 | 0.7510 | 0.0000 | 34.2627 | 1.0271 | 1.0820 | 1.0683 |
| MOOD | 0.9866 | 0.8349 | 0.0227 | 39.3981 | 1.4961 | 1.7603 | 1.4748 |
| GraphDiT | 0.8245 | 0.8712 | 0.9600 | 6.6443 | 0.7440 | 0.8857 | 0.7550 |
| MDMs | 18.61 | 0.88 44 | 0.2370 | 29.32 | 1.9846 | 1.6468 | 1.8290 |
| **Ours** | 0.9878 | 0.8569 | **0.9642** | **5.9219** | **0.6152** | **0.7010** | **0.6010** |

Furthermore, the visualizations illustrate the efficacy of our proposed approach: for both cyclic and branched molecules, the generated structures exhibit predominantly green atoms and bonds, indicating high chemical validity across diverse motifs. In contrast to the earlier failures observed in cyclic regions, MDMs with our approach now accurately reconstructs even complex ring systems.

<div class="row mt-3">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.html path="assets/img/2025-05-31-ai810/3.png" class="img-fluid rounded z-depth-2" %}
    </div>
</div>

## 6 Summary

The assumption that all molecular structures are equally amendable for molecular diffusion models is a simplification that overlooks the key challenges posed by structural complexity. By introducing a learnable, molecular structure-wise adaptive noise schedule, we enable diffusion models to account for these differences, leading to more accurate and chemically valid molecule generation.